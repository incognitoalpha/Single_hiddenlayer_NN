# -*- coding: utf-8 -*-
"""Single_hidden_layer_NN.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qgp93rDGN_D5Hz-p1gtWTkKGvZIzURUI
"""

import matplotlib.pyplot as plt
import numpy as np
import random

X_train = np.loadtxt('train_X.csv', delimiter = ',').T
Y_train = np.loadtxt('train_label.csv', delimiter = ',').T
X_test = np.loadtxt('test_X.csv', delimiter = ',').T
Y_test = np.loadtxt('test_label.csv', delimiter = ',').T

print("shape of x train:", X_train.shape )
print("shape of y train:", Y_train.shape )
print("shape of x test:", X_test.shape )
print("shape of y test:", Y_test.shape )

"""#Displaying of a random image"""

index = random.randrange(0,X_train.shape[1])
plt.imshow(X_train[:,index].reshape(28,28), cmap = 'gray')
plt.show()

"""#Model
Initialization

w1 = (n1,n0)

w2 = (n2, n1)

b1 = (n1,1)

b2 = (n2,1)

#Initialising Activation Function
"""

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0,x)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # Stability trick
    return exp_x / np.sum(exp_x, axis=0, keepdims=True)

def derivative_tanh(x):
  return 1 -  np.power(np.tanh(x), 2)

def derivative_relu(x):
  return np.where(x>0,1,0)

"""#Initialising Parameters"""

def initialize_parameters(n_x,n_h,n_y):
  w1 = np.random.randn(n_h,n_x)*0.01
  w2 = np.random.randn(n_y,n_h)*0.01
  b1 = np.zeros((n_h,1))
  b2 = np.zeros((n_y,1))
  para_dict = {
      "w1":w1,
      "b1":b1,
      "w2":w2,
      "b2":b2
  }
  return para_dict

"""#Forward Propagation
A1 = f(W1*A0+B1)

A2 = f(w2*A1+B2)
"""

def f_prop(x,para_dict):
  w1 = para_dict["w1"]
  b1 = para_dict["b1"]
  w2 = para_dict["w2"]
  b2 = para_dict["b2"]
  z1 = np.dot(w1,x)+b1
  a1 = tanh(z1)
  z2 = np.dot(w2,a1)+b2
  a2 = softmax(z2)

  forw_prop = {
      "z1":z1,
      "a1":a1,
      "z2":z2,
      "a2":a2
  }
  return forw_prop

"""#Cost Function"""

def cost_f(a2, y, epsilon=1e-8):
    m = y.shape[1]
    cost = -(1/m) * np.sum(y * np.log(a2 + epsilon))  # Avoid log(0)
    return cost

"""#Back Propogation
dz2 = a2-y

dw2 = (1/m).dz2*A1T

dB2 = (1/m).sum(dz2,1)



dz1 = w2T.dz2*f1'(z1)

dw1 = (1/m)*dz1*A1T

dB1 = (1/m).sum(dz1,1)
"""

def back_prop(x,y,forw_prop,para_dict):
  m = y.shape[1]
  w2 = para_dict["w2"]
  a1 = forw_prop["a1"]
  a2 = forw_prop["a2"]

  w1 = para_dict["w1"]
  b1 = para_dict["b1"]
  b2 = para_dict["b2"]

  dz2 = a2-y
  dw2 = (1/m)*np.dot(dz2,a1.T)
  db2 = (1/m)*np.sum(dz2, axis =1 , keepdims = True) #keepdims to give proper shape

  dz1 = np.dot(w2.T,dz2)*derivative_tanh(a1)
  dw1 = (1/m)*np.dot(dz1,x.T)
  db1 = (1/m)*np.sum(dz1, axis =1 , keepdims = True) #keepdims to give proper shape

  grad_dict={
      "dw1":dw1,
      "dw2":dw2,
      "db1":db1,
      "db2":db2
  }
  return grad_dict

"""#Update Parameters
w1 = w1-lr*dw1

w2 = w2-lr*dw2

b1 = b1-lr*db1

b2 = b2-lr*db2
"""

def up_para(para_dict,grad_dict, lr):
  w1 = para_dict["w1"]
  b1 = para_dict["b1"]
  b2 = para_dict["b2"]
  w2 = para_dict["w2"]


  dw1 = grad_dict["dw1"]
  db1 = grad_dict["db1"]
  db2 = grad_dict["db2"]
  dw2 = grad_dict["dw2"]

  w1 = w1-lr*dw1
  w2 = w2-lr*dw2
  b1 = b1-lr*db1
  b2 = b2-lr*db2

  para_dict = {
      "w1":w1,
      "w2":w2,
      "b1":b1,
      "b2":b2
  }
  return para_dict

"""#Compute Model"""

def model(x,y,n_h,learning_rate,iteration):
  n_x  = x.shape[0]
  n_y  = y.shape[0]
  cost_list = []
  para_dict = initialize_parameters(n_x,n_h,n_y)
  for i in range(iteration):
    forw_prop = f_prop(x,para_dict)
    cost = cost_f(forw_prop['a2'],y)
    grad_dict = back_prop(x,y,forw_prop,para_dict)
    para_dict = up_para(para_dict,grad_dict,learning_rate)
    cost_list.append(cost)
    if(i%(iteration/10)==0):
      print("cost after", i, "iteration is:",cost)
  return para_dict,cost_list

iteration = 500
n_h = 1000
learning_rate = 0.03
para_dict,cost_list = model(X_train,Y_train,n_h=n_h,learning_rate= learning_rate,iteration=iteration)

t = np.arange(0,iteration)
plt.plot(t,cost_list)
plt.show()

"""#Accuracy on training"""

forw_prop = f_prop(X_train,para_dict)
a_out = forw_prop["a2"]
a_out = np.argmax(a_out, axis=0)
y_out = np.argmax(Y_train, axis=0)
acc = np.mean(a_out==y_out)*100
print("Training Accuracy is:", acc)

"""#Accuracy on Testing"""

forw_prop = f_prop(X_test,para_dict)
a_out = forw_prop["a2"]
a_out = np.argmax(a_out, axis=0)
y_out = np.argmax(Y_test, axis=0)
acc = np.mean(a_out==y_out)*100
print("Testing Accuracy is:", acc)

index = random.randrange(0,X_test.shape[1])
plt.imshow(X_test[:,index].reshape(28,28), cmap = 'gray')
plt.show()

forw_prop = f_prop(X_test[:,index].reshape(X_test.shape[0],1),para_dict)
a_out = forw_prop["a2"]
a_out = np.argmax(a_out, axis=0)

print("Our model says it's:",a_out[0])